<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="FreMIM">
  <meta property="og:title" content="FreMIM"/>
  <meta property="og:description" content="FreMIM: Fourier Transform Meets Masked Image Modeling for Medical Image Segmentation"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/video_t1.png" />
  <meta property="og:image:width" content="2412"/>
  <meta property="og:image:height" content="1394"/>


  <meta name="twitter:title" content="FreMIM">
  <meta name="twitter:description" content="FreMIM: Fourier Transform Meets Masked Image Modeling for Medical Image Segmentation">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/video_t1.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="Image-to-Video">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>FreMIM</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>

  
  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">FreMIM: Fourier Transform Meets Masked Image Modeling for Medical Image Segmentation</h1>
            <h3 class="title is-3">WACV 2024</h3>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://scholar.google.com/citations?user=75OyC-oAAAAJ&hl=zh-CN" target="_blank">Wenxuan Wang</a><sup>1</sup>,&nbsp;&nbsp;</span>
              <span class="author-block">
                <a href="https://scholar.google.com/citations?user=Q9Np_KQAAAAJ&hl=en" target="_blank">Jing Wang</a><sup>1</sup>,&nbsp;&nbsp;</span>
              <span class="author-block">
                <a href="https://scholar.google.com/citations?user=TuEwcZ0AAAAJ&hl=en" target="_blank">Chen Chen</a><sup>2</sup>,&nbsp;&nbsp;</span>
              <span class="author-block">
                <a href="https://scholar.google.com/citations?user=HkEiMMwAAAAJ&hl=zh-CN" target="_blank">Jianbo Jiao</a><sup>3</sup>,&nbsp;&nbsp;</span>
              <span class="author-block">
                <a href="https://scholar.google.com/citations?user=57DcovcAAAAJ&hl=zh-CN&oi=sra" target="_blank">Yuanxiu Cai</a><sup>1</sup>,&nbsp;&nbsp;</span>
              <span class="author-block">
                <a href="https://scholar.google.com/citations?user=EoNWyTcAAAAJ&hl=zh-CN&oi=ao" target="_blank">Shanshan Song</a><sup>1</sup>,&nbsp;&nbsp;</span>
              <span class="author-block">
                <a href="http://saee.ustb.edu.cn/quantijiaoshi/2015-05-12/69.html" target="_blank">Jiangyun Li</a><sup>1</sup>,&nbsp;&nbsp;</span>

                  </span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block"><sup>1</sup>University of Science and Technology Beijing</span>
                    <span class="author-block"><sup>2</sup>University of Central Florida</span>
                    <span class="author-block"><sup>3</sup>University of Birmingham</span>
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://openaccess.thecvf.com/content/WACV2024/html/Wang_FreMIM_Fourier_Transform_Meets_Masked_Image_Modeling_for_Medical_Image_WACV_2024_paper.html" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                    <!-- Video link -->
                    <span class="link-block">
                      <a href="" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fab fa-youtube"></i>
                      </span>
                      <span>video</span>
                    </a>
                  </span>

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/Rubics-Xuan/FreMIM" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2304.10864" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>



<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
             The research community has witnessed the powerful potential of self-supervised Masked Image Modeling (MIM), which enables the models capable of learning visual representation from unlabeled data. In this paper, to incorporate both the crucial global structural information and local details for dense prediction tasks, we alter the perspective to the frequency domain and present a new MIM-based framework named FreMIM for self-supervised pre-training to better accomplish medical image segmentation tasks. Based on the observations that the detailed structural information mainly lies in the high-frequency components and the high-level semantics are abundant in the low-frequency counterparts, we further incorporate multi-stage supervision to guide the representation learning during the pre-training phase. Extensive experiments on three benchmark datasets show the superior advantage of our FreMIM over previous state-of-the-art MIM methods. Compared with various baselines trained from scratch, our FreMIM could consistently bring considerable improvements to model performance.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->


<!-- Image carousel -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <h2 class="title is-3">Methodology</h2>
      <div class="item">
        <!-- Your image here -->
        <img src="static/images/Methodology.png" alt="MY ALT TEXT"/>
        <h2 class="content has-text-justified">
           The overall architecture of our proposed FreMIM. At first, the input medical image is corrupted by the foreground masking strategy and then fed into the encoder, which consists of several stages with a hierarchical structure. The captured feature maps at different stages (i.e., S<SUB>1</SUB>, S<SUB>2</SUB>, ... S<SUB>n</SUB>) are fused by a bilateral aggregation decoder to generate the aggregated high-level and low-level feature representations (i.e., A<SUB>high</SUB> and A<SUB>low</SUB>). For the fused feature of each semantic level, an FMB is applied respectively to learn its recessive information in the frequency domain, resulting in the acquired P<SUB>low</SUB> and P<SUB>high</SUB>. Finally, the low-pass and high-pass Fourier spectra are both adopted as the reconstruction target to better guide the model to capture local details and global information.
       </h2>
     </div>
    </div>
  </div>
</div>
</div>
</section>
<!-- End image carousel -->

<!-- Image carousel -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <h2 class="title is-3">Quantitative Results</h2>
      <div class="item">
        <!-- Your image here -->
        <img src="static/images/Experiment.png" alt="MY ALT TEXT"/>
        <h2 class="content has-text-justified">
           Focusing on solely exploiting the given training samples (\ie the pre-training data only includes the specific downstream datasets without introducing any extra data) for 2D medical image segmentation (e.g., solely BraTS 2019 is used for pre-training when evaluating brain tumor segmentation), extensive experiments on three benchmark datasets are conducted to fully verify the effectiveness of FreMIM. Note that the numbers between parenthesis represent the gains with respect to specific baselines trained from scratch, while the red and blue color denote accuracy increase and decrease respectively.
       </h2>
     </div>
    </div>
  </div>
</div>
</div>
</section>
<!-- End image carousel -->


  <!-- Image carousel -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <h2 class="title is-3">Qualitative Results</h2>
      <div class="item">
        <!-- Your image here -->
        <img src="static/images/Visualization.png" alt="MY ALT TEXT"/>
        <h2 class="content has-text-justified">
           We compare the segmentation performance of different self-supervised methods, including MAE, DINO, and FreMIM on the BraTS 2019 dataset with visualization results. As shown in the figure, our method promotes the detailed pixel delineation of brain tumors and obtains more accurate predictions.
       </h2>
     </div>
    </div>
  </div>
</div>
</div>
</section>
<!-- End image carousel -->



<!--BibTex citation -->
<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@inproceedings{wang2024fremim,
      title={FreMIM: Fourier Transform Meets Masked Image Modeling for Medical Image Segmentation},
      author={Wang, Wenxuan and Wang, Jing and Chen, Chen and Jiao, Jianbo and Cai, Yuanxiu and Song, Shanshan and Li, Jiangyun},
      booktitle={Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision},
      pages={7860--7870},
      year={2024}
}</code></pre>
  </div>
</section>


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<script>
bulmaCarousel.attach('#results-carousel11', {
    slidesToScroll: 1,
    slidesToShow: 2,
    infinite: true,
    autoplay: false,
});
bulmaCarousel.attach('#results-carousel22', {
    slidesToScroll: 1,
    slidesToShow: 2,
    infinite: true,
    autoplay: false,
});
bulmaCarousel.attach('#results-carousel33', {
    slidesToScroll: 1,
    slidesToShow: 2,
    infinite: true,
    autoplay: false,
});
</script>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
