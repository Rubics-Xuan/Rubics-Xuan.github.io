<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <title>Wenxuan Wang</title>
  
  <meta name="author" content="Wenxuan Wang">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/icon.png">
</head>

<body>
  <table style="width:100%;max-width:850px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:60%;vertical-align:middle">
              <p style="text-align:center">
                <name>Wenxuan Wang (王文轩)</name>
              </p>
              <p>
              I am currently a third-year PhD student at the <a href="http://english.ia.cas.cn/" target="_blank">Institute of Automation, Chinese Academy of Sciences</a>, co-supervised with the <a href="https://www.baai.ac.cn/en/" target="_blank">Beijing Academy of Artificial Intelligence</a> by Prof. <a href="https://nlpr.ia.ac.cn/iva/liujing/" target="_blank">Jing Liu</a> and Dr. <a href="https://www.xloong.wang/" target="_blank">Xinlong Wang</a>.
              </p>  
              <p>
              My research interests span <strong>Foundation Models</strong>, <strong>Native Multimodal Models</strong>, <strong>Generative Models</strong>, and <strong>Visual Grounding</strong>.
              </p>
              <p style="text-align:center">
                <a href="mailto:wangwenxuan2023@ia.ac.cn">Email</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?hl=zh-CN&user=75OyC-oAAAAJ"> Google Scholar</a> &nbsp/&nbsp
                <a href="https://github.com/Rubics-Xuan"> Github </a> &nbsp/&nbsp
                <a href="data/CV_Wenxuan_Wang.pdf">Curriculum Vitae</a>
              </p>
            </td>
            <td style="padding:2.5%;width:30%;max-width:30%">
              <img style="width:80%;max-width:80%" alt="profile photo" src="img/wwx.png">
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>Education</heading>
            <p>
              <li style="margin: 5px;" >
                <b>University of Science and Technology Beijing,</b> Sept. 2016 - Jun. 2020 
                <strong>B.S.</strong> in School of Automation.
              <li style="margin: 5px;" >
                <b>University of Science and Technology Beijing,</b> Sept. 2020 - Jun. 2023 
                <strong>M.S.</strong> in School of Automation.
              <li style="margin: 5px;" >
                <b>Institute of Automation, Chinese Academy of Sciences,</b> Sept. 2023 - Jun. 2026 
                <strong>Ph.D.</strong> in Zidongtaichu Foundation Model Research Center.
              </li>
                <b>Beijing Academy of Artificial Intelligence,</b> Sept. 2023 - Jun. 2026 
                <strong>Ph.D.</strong> in Multimodal Large Model Research Center.
              </li>
            </p>
          </td>
        </tr>
      </tbody></table>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Recent Projects</heading>
              <p>
                * indicates equal contribution
              </p>
            </td>
          </tr>
        </tbody></table>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody></tbody>
          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
                <img style="width:100%;max-width:100%" src="img/emu3p5.png" alt="dise">
            </td>
            <td width="75%" valign="center">
                <papertitle>Emu3.5: Native Multimodal Models are World Learners</papertitle>
                <br>
                <a>Yufeng Cui*</a>, <a>Honghao Chen*</a>, <a>Haoge Deng*</a>, <a>Xu Huang*</a>, <a>Xinghang Li*</a>, <a>Jirong Liu*</a>, <a>Yang Liu*</a>, <a>Zhuoyan Luo*</a>, <a>Jinsheng Wang*</a>, <strong>Wenxuan Wang*</strong>, <a>Yueze Wang*</a>, <a>Chengyuan Wang*</a>, <a>Fan Zhang*</a>, <a>Yingli Zhao*</a>, <a>Ting Pan</a>, <a>Xianduo Li</a>, <a>Zecheng Hao</a>, <a>Wenxuan Ma</a>, <a>Zhuo Chen</a>, <a>Yulong Ao</a>, <a>Tiejun Huang</a>, <a>Zhongyuan Wang</a>, <a>Xinlong Wang</a>
                <br>
                <em>arXiv, 2025
                <br>
                <a href="https://arxiv.org/abs/2510.26583">[paper]</a> <a href="https://emu.world/">[Page]</a> <a href="https://github.com/baaivision/Emu3.5">[Code]</a>
                <br>
                <p> Emu3.5, a large-scale multimodal world model that natively predicts the next state across vision and language </p>
            </td>
            </tr>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>First-author Publications</heading>
              <p>
                * indicates equal contribution
              </p>
            </td>
          </tr>
        </tbody></table>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody></tbody>
          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
                <img style="width:100%;max-width:100%" src="img/ett.png" alt="dise">
            </td>
            <td width="75%" valign="center">
                <papertitle>End-to-End Vision Tokenizer Tuning</papertitle>
                <br>
                <strong>Wenxuan Wang*</strong>, <a>Fan Zhang*</a>, <a>Yufeng Cui*</a>, <a>Haiwen Diao*</a>, <a>Zhuoyan Luo</a>, <a>Huchuan Lu</a>, <a>Jing Liu</a>, <a>Xinlong Wang</a>
                <br>
                <em>NeurIPS, 2025
                <br>
                <a href="https://arxiv.org/abs/2505.10562">[paper]</a>
                <br>
                <p> an end-to-end vision tokenizer tuning approach that enables joint optimization between vision tokenization and target autoregressive tasks </p>
            </td>
            </tr>

          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
                <img style="width:100%;max-width:100%" src="img/unires++.png" alt="dise">
            </td>
            <td width="75%" valign="center">
                <papertitle>Towards Unified Referring Expression Segmentation Across Omni-Level Visual Target Granularities</papertitle>
                <br>
                <a>Jing Liu*</a>, <strong>Wenxuan Wang*</strong>, <a>Yisi Zhang</a>, <a>Yepeng Tang</a>, <a>Xingjian He</a>, <a>Longteng Guo</a>, <a>Tongtian Yue</a>, <a>Xinlong Wang</a>
                <br>
                <em>arXiv, 2025
                <br>
                <a href="https://arxiv.org/abs/2504.01954">[paper]</a>
                <br>
                <p> takes a step further towards visual granularity unified RES task </p>
            </td>
            </tr>


          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
                <img style="width:100%;max-width:100%" src="img/idg.png" alt="dise">
            </td>
            <td width="75%" valign="center">
                <papertitle>Image Difference Grounding with Natural Language</papertitle>
                <br>
                <strong>Wenxuan Wang*</strong>, <a>Zijia Zhao*</a>, <a>Yisi Zhang*</a>, <a>Yepeng Tang</a>, <a>Erdong Hu</a>, <a>Xinlong Wang</a>, <a>Jing Liu</a>
                <br>
                <em>arXiv, 2025
                <br>
                <a href="https://arxiv.org/abs/2504.01952">[paper]</a>
                <br>
                <p> push towards precisely localizing visual differences based on user instructions </p>
            </td>
            </tr>

          
          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
                <img style="width:100%;max-width:100%" src="img/diva.png" alt="dise">
            </td>
            <td width="75%" valign="center">
                <papertitle>Diffusion Feedback Helps CLIP See Better</papertitle>
                <br>
                <strong>Wenxuan Wang*</strong>, <a>Quan Sun*</a>, <a>Fan Zhang</a>, <a>Yepeng Tang</a>, <a>Jing Liu</a>, <a>Xinlong Wang</a>
                <br>
                <em>ICLR, 2025
                <br>
                <a href="https://arxiv.org/abs/2407.20171">[paper]</a> <a href="https://rubics-xuan.github.io/DIVA/">[Page]</a> <a href="https://github.com/baaivision/DIVA">[Code]</a>
                <br>
                <p> leverages generative feedback from text-to-image diffusion models to optimize CLIP representations, with only images (without corresponding text) </p>
            </td>
            </tr>
          
          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
                <img style="width:100%;max-width:100%" src="img/ivg.png" alt="dise">
            </td>
            <td width="75%" valign="center">
                <papertitle>Beyond Literal Descriptions: Understanding and Locating Open-World Objects Aligned with Human Intentions</papertitle>
                <br>
                <strong>Wenxuan Wang*</strong>, <a>Yisi Zhang*</a>, <a>Xingjian He</a>, <a>Yichen Yan</a>, <a>Zijia Zhao</a>, <a>Xinlong Wang</a>, <a>Jing Liu</a>
                <br>
                <em>ACL, 2024 (Findings)
                <br>
                <a href="https://arxiv.org/abs/2402.11265">[paper]</a>
                <br>
                <p> takes a step further to the intention-driven visual-language understanding and promotes classic visual grounding towards human intention interpretation </p>
            </td>
            </tr>

          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
                <img style="width:100%;max-width:100%" src="img/mres.png" alt="dise">
            </td>
            <td width="75%" valign="center">
                <papertitle>Unveiling Parts Beyond Objects: Towards Finer-Granularity Referring Expression Segmentation</papertitle>
                <br>
                <strong>Wenxuan Wang*</strong>, <a>Tongtian Yue*</a>, <a>Yisi Zhang</a>, <a>Longteng Guo</a>, <a>Xingjian He</a>, <a>Xinlong Wang</a>, <a>Jing Liu</a>
                <br>
                <em>CVPR, 2024
                <br>
                <a href="https://arxiv.org/abs/2312.08007">[paper]</a> <a href="https://rubics-xuan.github.io/MRES/">[Page]</a> <a href="https://github.com/Rubics-Xuan/MRES">[Code]</a>
                <br>
                <p> takes a step further to finer-grained part-level referring expression segmentation task </p>
            </td>
            </tr>
          
          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
                <img style="width:100%;max-width:100%" src="img/cm-masksd.png" alt="dise">
            </td>
            <td width="75%" valign="center">
                <papertitle>CM-MaskSD: Cross-Modality Masked Self-Distillation for Referring Image Segmentation</papertitle>
                <br>
                <strong>Wenxuan Wang</strong>, <a>Jing Liu</a>, <a>Xingjian He</a>, <a>Yisi Zhang</a>, <a>Chen Chen</a>, <a>Jiachen Shen</a>, <a>Yan Zhang</a>, <a>Jiangyun Li</a>
                <br>
                <em>IEEE-TMM, 2024
                <br>
                <a href="https://arxiv.org/abs/2305.11481">[paper]</a>
                <br>
                <p> a new cross-modality masked self-distillation framework for referring image segmentation task </p>
            </td>
            </tr>
        

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Co-author Publications</heading>
              <p>
                * indicates equal contribution
              </p>
            </td>
          </tr>
        </tbody></table>
  
  
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody></tbody>
          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
                <img style="width:100%;max-width:100%" src="img/evev2.png" alt="dise">
            </td>
            <td width="75%" valign="center">
                <papertitle>EVEv2: Improved Baselines for Encoder-Free Vision-Language Models</papertitle>
                <br>
                <a>Haiwen Diao*</a>, <a>Xiaotong Li*</a>, <a>Yufeng Cui*</a>, <a>Yueze Wang*</a>, <a>Haoge Deng</a>, <a>Ting Pan</a>, <strong>Wenxuan Wang</strong>, <a>Huchuan Lu</a>, <a>Xinlong Wang</a>
                <br>
                <em>ICCV, 2025 (highlight)
                <br>
                <a href="https://arxiv.org/abs/2502.06788">[paper]</a> <a href="https://github.com/baaivision/EVE">[Code]</a>
                <br>
                <p> encoder-free vision-language models </p>
            </td>
            </tr>


          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
                <img style="width:100%;max-width:100%" src="img/univla.png" alt="dise">
            </td>
            <td width="75%" valign="center">
                <papertitle>Unified Vision-Language-Action Model</papertitle>
                <br>
                <a>Yuqi Wang</a>, <a>Xinghang Li</a>, <strong>Wenxuan Wang</strong>, <a>Junbo Zhang</a>, <a>Yingyan Li</a>, <a>Yuntao Chen</a>, <a>Xinlong Wang</a>, <a>Zhaoxiang Zhang</a>
                <br>
                <em>arXiv, 2025
                <br>
                <a href="https://arxiv.org/abs/2506.19850">[paper]</a> <a href="https://robertwyq.github.io/univla.github.io">[Page]</a> <a href="https://github.com/baaivision/UniVLA">[Code]</a>
                <br>
                <p> unified vision-language-action model for embodied intelligence </p>
            </td>
            </tr>


            
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Honors and Awards</heading>
              <p>
                <li style="margin: 5px;">2025 National Scholarship (Ph.D.)</li>
                <li style="margin: 5px;">2022 National Scholarship (Master)</li>
              </p>
            </td>
          </tr>
        </tbody></table>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                <a href="https://jonbarron.info/">Website Template</a>
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
  
<p><center>
	  <div id="clustrmaps-widget" style="width:5%">
    <script type="text/javascript" id="clstr_globe" src="//clustrmaps.com/globe.js?d=pYUe-9vNee5nJl9ztd0lgo-xYiNaKRYwhjvT3xnX5Mg"></script>
	  </div>        
	  <br>
	    &copy; Wenxuan Wang | Last updated: November 3, 2025
</center></p>
</body>

</html>